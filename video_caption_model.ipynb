{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKL6MrWD9-iD"
      },
      "source": [
        "This notebook is part of an undergraduate project on multilingual video captioning. It demonstrates the system pipeline, model design, and evaluation logic. Results are discussed qualitatively due to the multilingual and generative nature of the task. The code is shared for transparency and educational purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5vU49sYXFjP",
        "outputId": "bcbe00cd-5e6e-473e-a59d-9c2e4ebda60d"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2ekYoNsl8My",
        "outputId": "ee2a5407-c7b0-4ae5-e6f1-7e2f30952a1f"
      },
      "outputs": [],
      "source": [
        "%cd \"/content/drive/MyDrive/Capstone\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH-x5gc6wqgY"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voK3FwfQVG_N"
      },
      "source": [
        "# **Part 1: Using Whisper for Transcription**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfBU_J5PVQtA"
      },
      "source": [
        "## Step 1: Install Whisper and FFmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "O1UZ84EwVBfH",
        "outputId": "91e223ee-e163-48b7-888d-cffdfd50b49f"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iPwdwksSqJZ"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvUuh4bUVuEB"
      },
      "source": [
        "## Step 2: Loading and Transcribing the Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWvX9yUYV0hF",
        "outputId": "6872c872-aced-40e7-a4bd-e2480a52c04f"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "import subprocess\n",
        "\n",
        "\n",
        "# Extracting audio from video\n",
        "\n",
        "video_path = \"/content/drive/MyDrive/Capstone/videoplayback.mp4\"\n",
        "audio_path = \"/content/drive/MyDrive/Capstone/videoplayback.mp3\"\n",
        "\n",
        "subprocess.run([\"ffmpeg\", \"-i\", video_path, \"-ar\", \"16000\", \"-ac\", \"1\", audio_path])\n",
        "\n",
        "# load whisper model\n",
        "model = whisper.load_model(\"medium\")\n",
        "\n",
        "# transcribe audio\n",
        "result = model.transcribe(audio_path, language=\"en\")\n",
        "\n",
        "# print the result\n",
        "print(result[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0VXqAMp5sjJ"
      },
      "source": [
        "# Step 3: Saving the Transcription to a text file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgibY1_530j3"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/Capstone/videoplayback.txt\", \"w\") as file:\n",
        "  file.write(result[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmZOyvz15z-T"
      },
      "source": [
        "# Step 4: Save Transcription to a JSON File with Timestamps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFYVnIEK4_-7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# saving the result with timestamps as JSON\n",
        "with open(\"/content/drive/MyDrive/Capstone/videoplayback.json\", \"w\") as file:\n",
        "  json.dump(result, file, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLrKb0hzlcVc"
      },
      "source": [
        "# Step 5: Saving the Transcription in a Subtitle Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoZZM4-BltEn"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/Capstone/videoplayback.srt\", \"w\") as file:\n",
        "  for i, segment in enumerate(result[\"segments\"]):\n",
        "\n",
        "    # Converting the starting and ending times to Subtitle format in (HH:MM:SS)\n",
        "    start = segment[\"start\"]\n",
        "    end = segment[\"end\"]\n",
        "    text = segment[\"text\"]\n",
        "\n",
        "    # Formating time in SRT format\n",
        "    start_srt = f\"{int(start // 3600):02}:{int((start % 3600) // 60):02}:{int(start % 60):02}, {int((start % 1) * 1000):03}\"\n",
        "    end_srt = f\"{int(end // 3600):02}:{int((end % 3600) // 60):02}:{int(end % 60):02}, {int((end % 1) * 1000):03}\"\n",
        "\n",
        "    # Writing to SRT file\n",
        "    file.write(f\"{i+1}\\n\")\n",
        "    file.write(f\"{start_srt} --> {end_srt}\\n\")\n",
        "    file.write(f\"{text}\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAYibbiHxdYF"
      },
      "source": [
        "# **Model Building**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "22qziiHVg84I",
        "outputId": "8d6a40b8-73b2-47b5-dfe7-7f83d113093d"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MVhrFT1-Ll2"
      },
      "source": [
        "# **Part 2: Integrating Captions with video frames**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC10GFxo-Pys"
      },
      "source": [
        "# Step 1: Frame Extraction and Caption Alignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CcrFf_I789Q",
        "outputId": "2e3c3a7e-53c6-42c7-a8de-310c12543a86"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import json\n",
        "\n",
        "# Loading the transcription data\n",
        "with open(\"/content/drive/MyDrive/Capstone/videoplayback.json\", \"r\") as file:\n",
        "  transcription_data = json.load(file)\n",
        "\n",
        "# Defining the frame extraction interval in seconds\n",
        "frame_interval = 2\n",
        "\n",
        "# Opening the video file\n",
        "\n",
        "video_path = \"/content/drive/MyDrive/Capstone/videoplayback.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# Frame counter\n",
        "frames_data = []\n",
        "frame_count = 0\n",
        "\n",
        "# Processing the video and align frames with captions\n",
        "while cap.isOpened():\n",
        "  ret, frame = cap.read()\n",
        "  if not ret:\n",
        "    break\n",
        "\n",
        "  # Extracting frame every \"frame_interval\" seconds\n",
        "  if frame_count % int(fps * frame_interval) == 0:\n",
        "    timestamp = frame_count / fps\n",
        "\n",
        "    # Finding the corresponding caption segment based on timestamp\n",
        "    caption = \"\"\n",
        "    for segment in transcription_data[\"segments\"]:\n",
        "      if segment[\"start\"] <= timestamp < segment[\"end\"]:\n",
        "        caption = segment[\"text\"]\n",
        "        break\n",
        "\n",
        "    # Saving frame and corresponding caption\n",
        "    frames_data.append({\"frame\": frame, \"caption\": caption, \"timestamp\": timestamp})\n",
        "\n",
        "  frame_count += 1\n",
        "\n",
        "# Releasing the video capture\n",
        "cap.release()\n",
        "\n",
        "# Example: Access first frame and its caption\n",
        "print(frames_data[0][\"caption\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r6lOu3e-DIR"
      },
      "source": [
        "# Next Steps: Preparing Frames and Captions for the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUGbNxE6-EGp"
      },
      "source": [
        "## Step 1: Frame Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v24ktRjc9z3k",
        "outputId": "ae22886c-09b4-4194-809a-3605c854779b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Setting frame dimensions for resizing\n",
        "frame_height, frame_width = 224, 224\n",
        "\n",
        "# Preprocess frames\n",
        "processed_frames = []\n",
        "for data in frames_data:\n",
        "  frame = data[\"frame\"]\n",
        "\n",
        "  # Resizing and normalizing the frame\n",
        "  frame_resized = cv2.resize(frame, (frame_width, frame_height))\n",
        "  frame_normalized = frame_resized / 255.0\n",
        "\n",
        "  processed_frames.append(frame_normalized)\n",
        "\n",
        "# Converting to numpy array for model compatibility\n",
        "processed_frames = np.array(processed_frames)\n",
        "print(\"Processed Frames Shape:\", processed_frames.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibtmBNEW9-Mu"
      },
      "source": [
        "## Step 2: Converting Captions to mBERT Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AzAvxNrZhWyt",
        "outputId": "79f8a139-174f-422f-8c64-fe5bc8ab23a4"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OC5PYPd0d1Pj",
        "outputId": "f8e46773-bc76-4f61-b60f-d8f54feac847"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow --upgrade  # For CPU version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLv56ybkG9mR",
        "outputId": "a0d4b044-4752-42dc-9a32-94c21f4f0224"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "KfH9HFr9iH_F",
        "outputId": "182bbcfd-1483-4cb7-c6a7-631a22c3d654"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Ensure tensorflow-text is compatible with the installed TensorFlow version\n",
        "!pip install -U transformers huggingface_hub safetensors\n",
        "!pip install tf-keras tensorflow-text==2.19.0 # Explicitly install compatible tensorflow-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "a254890c7e274807bb4742456f34c608",
            "08e5206a2117494494aff64c701a2c69",
            "945f5677fddf42a498a32c06f30f4c56",
            "977488c249554c188753358b4dbf3d1e",
            "bfcd8fee799b46e0985cfd43e8b0e786",
            "803c87c5a22e4490a274195ac18f1276",
            "76d1354e5ffb4bc08efa2887cf8e5262",
            "a03d500d35f746cbbd1ab0124456f258",
            "b403ee9a41e94056af758bc3f3fe5975",
            "ad5af77ba2eb4d8b9a52804d10e26bda",
            "0a17615dd20d4b688420d7aedbddd0c1"
          ]
        },
        "collapsed": true,
        "id": "l8IOl-zf90-Y",
        "outputId": "f0bc18a0-0f4f-4407-fe89-7c42108db1f5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "\n",
        "# Loading mBERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "# Explicitly load from PyTorch weights and disable safetensors\n",
        "model = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\", from_pt=True, use_safetensors=False)\n",
        "\n",
        "# Preprocessing and embed captions\n",
        "embedded_captions = []\n",
        "for data in frames_data:\n",
        "  caption = data[\"caption\"]\n",
        "\n",
        "  # Tokenizing and converting to mBERT embedding\n",
        "  inputs = tokenizer(caption, return_tensors=\"tf\", padding=\"max_length\", truncation=True, max_length=20)\n",
        "  outputs = model(**inputs)\n",
        "\n",
        "  # Use the last hidden state for the embedding\n",
        "  embedding = outputs.last_hidden_state[:, 0, :]\n",
        "  embedded_captions.append(embedding)\n",
        "\n",
        "# Converting list to tensor\n",
        "embedded_captions = tf.concat(embedded_captions, axis=0)\n",
        "print(\"Embedded Captions Shape:\", embedded_captions.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKg8ntjABBpR",
        "outputId": "3eabdde4-9dbc-47e2-f153-22da54cd6e0e"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained(\"saved_models/tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVNBbautEFNw"
      },
      "source": [
        "# Day 5: Creating Sequence for the **model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4tSIa-5EKsG"
      },
      "source": [
        "## Step 1: Sequence Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8vnzXjsER0G",
        "outputId": "48da9847-bb05-45ec-aa91-9d7def2d5eb0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Setting the sequence length (10 frames per sequence)\n",
        "sequence_length = 10\n",
        "\n",
        "# Preparing the frame and caption sequences\n",
        "frame_sequences = []\n",
        "caption_sequences = []\n",
        "\n",
        "for i in range(len(processed_frames) - sequence_length + 1):\n",
        "  frame_sequence = processed_frames[i:i + sequence_length]\n",
        "  caption_sequence = embedded_captions[i + sequence_length - 1]\n",
        "\n",
        "  frame_sequences.append(frame_sequence)\n",
        "  caption_sequences.append(caption_sequence)\n",
        "\n",
        "# Converting to numpy arrays for model compatibility\n",
        "frame_sequences = np.array(frame_sequences)\n",
        "caption_sequences = np.array(caption_sequences)\n",
        "print(\"Frame Sequences Shape:\", frame_sequences.shape)\n",
        "print(\"Caption Sequences Shape:\", caption_sequences.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi6V9KplFqM_"
      },
      "source": [
        "## Step 2: Defining and Compiling the LSTM-based CaptioningModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "zukScQARFyBo",
        "outputId": "0d426884-87ed-4bf4-985d-5a1bba1fa446"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Defining the model architecture\n",
        "input_shape = (sequence_length, frame_height, frame_width, 3)\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "# Input layer for frame sequences\n",
        "model.add(layers.TimeDistributed(layers.Conv2D(32, (3, 3), activation='relu'), input_shape=input_shape))\n",
        "model.add(layers.TimeDistributed(layers.MaxPooling2D((2, 2))))\n",
        "model.add(layers.TimeDistributed(layers.Conv2D(64, (3, 3), activation='relu')))\n",
        "model.add(layers.TimeDistributed(layers.MaxPooling2D((2, 2))))\n",
        "model.add(layers.TimeDistributed(layers.Flatten()))\n",
        "\n",
        "# LSTM layer to process frame caption\n",
        "model.add(layers.LSTM(256, return_sequences=False))\n",
        "\n",
        "# Dense layer to output caption embedding\n",
        "model.add(layers.Dense(768, activation='linear')) # 768 is the size of the mBERT embedding\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0efZWxzFIIA0"
      },
      "source": [
        "## Step 3: Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GNJ-3fTeILrx",
        "outputId": "d0e5c9df-de1b-48ed-8cf3-01b7dbffdc53"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"saved_models\", exist_ok=True)\n",
        "# Setting the training parameters\n",
        "\n",
        "batch_size = 4 # Further reduced batch size to mitigate OOM error\n",
        "epochs = 20\n",
        "validation_split = 0.2\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(\n",
        "  frame_sequences,\n",
        "  caption_sequences,\n",
        "  batch_size=batch_size,\n",
        "  epochs=epochs,\n",
        "  validation_split=validation_split,\n",
        "  shuffle=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JupV4YxzMfz",
        "outputId": "22535dbe-3059-4382-cc8a-a95f07b11bae"
      },
      "outputs": [],
      "source": [
        "model.save(\"saved_models/video_caption_model.h5\")  # or use .h5 if preferred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_CSq0NdaiCm"
      },
      "outputs": [],
      "source": [
        "model.save(\"saved_models/video_caption_model.keras\")  # or use .h5 if preferred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXKREWEqImxg"
      },
      "source": [
        "### Plotting the Training and Validation Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Ge7rkbcLIkM5",
        "outputId": "1e88527b-33f7-4452-b2ef-5f759db33bdb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdY9BMzMOPFc"
      },
      "source": [
        "## Step 4: Model Evaluation\n",
        "Let's evaluate the performance of the model trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eEV0Za_OTkU",
        "outputId": "5980d179-0b1c-4192-9c23-874624837743"
      },
      "outputs": [],
      "source": [
        "# Let's assume the 'test_frame_sequences' and 'test_caption_sequences'\n",
        "# are prepared similarly as training data\n",
        "# Set the batch size for evaluation, same as training to prevent OOM\n",
        "batch_size_eval = 4\n",
        "test_loss, test_mse = model.evaluate(frame_sequences, caption_sequences, batch_size=batch_size_eval)\n",
        "print(f\"Test Loss (MSE): {test_loss}, Test MSE: {test_mse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rznA0rpPP4oy"
      },
      "source": [
        "## Step 5: Generating Captions from Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "N7y6iOeQP9pI",
        "outputId": "9ebba404-cd2b-4b5e-81e7-af96708a4da6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def get_top_k_captions(embedding, reference_captions, reference_embeddings, k=3):\n",
        "    # Reshape embedding to 2D if needed\n",
        "    embedding = embedding.reshape(1, -1) if embedding.ndim == 1 else embedding\n",
        "\n",
        "    # Calculate cosine similarity scores\n",
        "    cosine_similarities = cosine_similarity(embedding, reference_embeddings).flatten()\n",
        "    # Get top-k indices\n",
        "    top_k_indices = np.argsort(cosine_similarities)[-k:][::-1]\n",
        "    # Return top-k captions and their similarity scores\n",
        "    top_k_captions = [reference_captions[i] for i in top_k_indices]\n",
        "    top_k_scores = cosine_similarities[top_k_indices]\n",
        "    return list(zip(top_k_captions, top_k_scores))\n",
        "\n",
        "def beam_search_captions(predicted_embeddings, reference_captions, reference_embeddings, beam_width=3):\n",
        "    all_generated_captions = []\n",
        "\n",
        "    for embedding in predicted_embeddings:\n",
        "        top_k_captions = get_top_k_captions(embedding, reference_captions, reference_embeddings, k=beam_width)\n",
        "        all_generated_captions.append(top_k_captions)\n",
        "\n",
        "    # Get the highest scored caption across all beams\n",
        "    best_caption_sequence = max(all_generated_captions, key=lambda x: np.mean([score for _, score in x]))\n",
        "    return [caption for caption, _ in best_caption_sequence]\n",
        "\n",
        "# Generate predictions for frame sequences\n",
        "predicted_embeddings = model.predict(frame_sequences)\n",
        "print(\"Predicted Embeddings Shape:\", predicted_embeddings.shape)\n",
        "\n",
        "# Defining the reference captions and embeddings\n",
        "reference_captions = [data[\"caption\"] for data in frames_data]\n",
        "reference_embeddings = embedded_captions\n",
        "\n",
        "# Generate captions using beam search\n",
        "generated_captions_beam_search = beam_search_captions(predicted_embeddings, reference_captions, reference_embeddings)\n",
        "\n",
        "# Display the results\n",
        "for i, caption in enumerate(generated_captions_beam_search[:5]):  # Displaying first 5 generated captions\n",
        "    print(f\"Generated Caption {i+1}: {caption}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzodlLZNQmur"
      },
      "source": [
        "# **Evaluating Model-Generated Captions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylEfJ8JDQydr"
      },
      "source": [
        "## Step 1.1: Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8FqRM1GDQ6ki",
        "outputId": "e5f7df4b-002f-44d7-c16d-4b50bc53e5be"
      },
      "outputs": [],
      "source": [
        "!pip install nltk rouge-score\n",
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ja__RC--RAxk",
        "outputId": "4b7f1029-afd0-4154-8b56-c966ee3aa933"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Downloading the required NLTK data for METEOR\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcdnPsdGRfLG"
      },
      "source": [
        "## Step 1.2: Preparing the Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xteKSRhBRoe6"
      },
      "source": [
        "## Step 1.2: Calculating BLEU, METEOR, and ROUGE Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nG8GcQ2zReJz",
        "outputId": "47a1cc19-f584-4704-86e9-40d35824a628"
      },
      "outputs": [],
      "source": [
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Storing scores\n",
        "bleu_scores = []\n",
        "meteor_scores = []\n",
        "rouge_scores = []\n",
        "\n",
        "for ref, gen in zip(reference_captions, generated_captions_beam_search):\n",
        "  # Calculating BLEU score\n",
        "  bleu_score = sentence_bleu([ref.split()], gen.split())\n",
        "  bleu_scores.append(bleu_score)\n",
        "\n",
        "  # Calculating METEOR score\n",
        "  meteor = meteor_score([ref.split()], gen.split())\n",
        "  meteor_scores.append(meteor)\n",
        "\n",
        "  # Calculating ROUGE score\n",
        "  rouge_score = scorer.score(ref, gen)\n",
        "  rouge_scores.append(rouge_score)\n",
        "\n",
        "# Displaying the average scores for an overall view\n",
        "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "avg_meteor = sum(meteor_scores) / len(meteor_scores)\n",
        "avg_rouge1 = sum(score['rouge1'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "avg_rouge2 = sum(score['rouge2'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "avg_rougeL = sum(score['rougeL'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "\n",
        "# Printing the average\n",
        "print(\"Average BLEU Score:\", avg_bleu)\n",
        "print(\"Average METEOR Score:\", avg_meteor)\n",
        "print(\"Average ROUGE-1 Score:\", avg_rouge1)\n",
        "print(\"Average ROUGE-2 Score:\", avg_rouge2)\n",
        "print(\"Average ROUGE-L Score:\", avg_rougeL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "pbNkWilL54bo",
        "outputId": "b7301ecd-f56e-49f3-d055-a65c55ec9a5e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you want to display BLEU, METEOR, ROUGE-1, ROUGE-2, and ROUGE-L scores\n",
        "metric = ['BLEU', 'METEOR', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
        "scores = [avg_bleu, avg_meteor, avg_rouge1, avg_rouge2, avg_rougeL]\n",
        "\n",
        "# Create bar chart\n",
        "plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
        "plt.bar(metric, scores, color=['blue', 'green', 'red', 'orange', 'purple'])\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Scores')\n",
        "plt.title('Average BLEU, METEOR, and ROUGE Scores')\n",
        "\n",
        "# Add value labels on top of bars\n",
        "for i, v in enumerate(scores):\n",
        "    plt.text(i, v + 0.01, str(round(v, 2)), ha='center', va='bottom')\n",
        "\n",
        "# Display the chart\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08e5206a2117494494aff64c701a2c69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_803c87c5a22e4490a274195ac18f1276",
            "placeholder": "​",
            "style": "IPY_MODEL_76d1354e5ffb4bc08efa2887cf8e5262",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "0a17615dd20d4b688420d7aedbddd0c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76d1354e5ffb4bc08efa2887cf8e5262": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "803c87c5a22e4490a274195ac18f1276": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "945f5677fddf42a498a32c06f30f4c56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a03d500d35f746cbbd1ab0124456f258",
            "max": 714314041,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b403ee9a41e94056af758bc3f3fe5975",
            "value": 714314041
          }
        },
        "977488c249554c188753358b4dbf3d1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad5af77ba2eb4d8b9a52804d10e26bda",
            "placeholder": "​",
            "style": "IPY_MODEL_0a17615dd20d4b688420d7aedbddd0c1",
            "value": " 714M/714M [00:05&lt;00:00, 138MB/s]"
          }
        },
        "a03d500d35f746cbbd1ab0124456f258": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a254890c7e274807bb4742456f34c608": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_08e5206a2117494494aff64c701a2c69",
              "IPY_MODEL_945f5677fddf42a498a32c06f30f4c56",
              "IPY_MODEL_977488c249554c188753358b4dbf3d1e"
            ],
            "layout": "IPY_MODEL_bfcd8fee799b46e0985cfd43e8b0e786"
          }
        },
        "ad5af77ba2eb4d8b9a52804d10e26bda": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b403ee9a41e94056af758bc3f3fe5975": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bfcd8fee799b46e0985cfd43e8b0e786": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
